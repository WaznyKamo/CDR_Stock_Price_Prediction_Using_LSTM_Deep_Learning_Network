{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dated-struggle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impossible-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16296623547421031147\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 8541087468\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2982853667988777961\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intellectual-kuwait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complex-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-disposal",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rising-image",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: acp_d.csv\n",
      "Loaded file: alr_d.csv\n",
      "Loaded file: ccc_d.csv\n",
      "Loaded file: cdr_d.csv\n",
      "Loaded file: dnp_d.csv\n",
      "Loaded file: kgh_d.csv\n",
      "Loaded file: pkn_d.csv\n"
     ]
    }
   ],
   "source": [
    "directory = \"Testing_data/\"\n",
    "stock_data_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if len(filename) > 10:\n",
    "        continue\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "    stock_data = stock_data.rename(columns={'Data': 'Date', 'Otwarcie': 'Open', 'Najwyzszy': 'Highest', 'Najnizszy': 'Lowest', 'Zamkniecie': 'Close', 'Wolumen': 'Volume'})\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data.Date)\n",
    "    stock_data_list.append(stock_data)\n",
    "    print('Loaded file: ' + filename)\n",
    "\n",
    "for i in range(len(stock_data_list)):\n",
    "    stock_data_list[i] = stock_data_list[i].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(data, period):\n",
    "    data['MA'] = data['Close'].rolling(period).mean()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smaller-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RSI (data, period):\n",
    "    increase_difference, decrease_difference = data['Close'].diff(), data['Close'].diff()\n",
    "    increase_difference[increase_difference < 0] = 0\n",
    "    decrease_difference[decrease_difference > 0] = 0\n",
    "    roll_increase = increase_difference.ewm(span = period).mean()\n",
    "    roll_decrease = decrease_difference.abs().ewm(span = period).mean()\n",
    "    RS = roll_increase / roll_decrease\n",
    "    data['RSI'] = 100 - (100 / (1 + RS))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "respiratory-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ROC(data):\n",
    "    data['ROC'] = data['Close'].pct_change()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "young-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stochastic_oscillator(data, period):\n",
    "    L14, H14 = data['Close'].rolling(period).min(), data['Close'].rolling(period).max()\n",
    "    data['K'] = (data['Close'] - L14)/(H14 - L14)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constitutional-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SO_moving_average(data, so_period, ma_period):\n",
    "    L, H = data['Close'].rolling(so_period).min(), data['Close'].rolling(so_period).max()\n",
    "    K = (data['Close'] - L)/(H - L)\n",
    "    data['D'] = K.rolling(ma_period).mean()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blind-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MACD(data, period_long, period_short):\n",
    "    EMA_long = data['Close'].ewm(period_long).mean()\n",
    "    EMA_short = data['Close'].ewm(period_short).mean()\n",
    "    data['MACD'] = EMA_short - EMA_long\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "italic-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MACD_histogram(data, period_long, period_short, period_signal):\n",
    "    EMA_long = data['Close'].ewm(period_long).mean()\n",
    "    EMA_short = data['Close'].ewm(period_short).mean()\n",
    "    MACD = EMA_short - EMA_long\n",
    "    MACD_signal = MACD.ewm(9).mean()\n",
    "    data['MACD_Histogram'] = MACD - MACD_signal\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bridal-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PPO(data, period_long, period_short):\n",
    "    EMA_long = data['Close'].ewm(period_long).mean()\n",
    "    EMA_short = data['Close'].ewm(period_short).mean()\n",
    "    data['PPO'] = (EMA_short - EMA_long)/EMA_long\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "taken-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TEMA(data, period):\n",
    "    SEMA = data['Close'].ewm(period).mean()\n",
    "    DEMA = SEMA.ewm(period).mean()\n",
    "    data['TEMA'] = DEMA.ewm(period).mean()\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "original-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CGI(data, period):\n",
    "    typical_price = (data['Highest'] + data['Lowest'] + data['Close']) / 3\n",
    "    MA = typical_price.rolling(period).mean()\n",
    "    mean_deviation = (MA - typical_price).abs().rolling(period).mean()\n",
    "    data['CCI'] = (typical_price - MA) / (0.15 * mean_deviation)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "apart-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Williams_Percent_Range(data, period):\n",
    "    data['Percent_Range'] = (data['Highest'].rolling(period).max() - data['Close']) / (data['Highest'].rolling(period).max() - data['Lowest'].rolling(period).min())\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "annual-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    list_of_features = []\n",
    "    for column in data.columns:\n",
    "        list_of_features.append(data[column])\n",
    "\n",
    "    dataset = np.transpose(list_of_features)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "    return scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indonesian-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_X_Y(data):\n",
    "    list_of_features = []\n",
    "    list_of_outputs = []\n",
    "    for column in data.columns:\n",
    "        list_of_features.append(data[column])\n",
    "        if column in ['Open', 'Close', 'Highest', 'Lowest', 'Volume']:\n",
    "            list_of_outputs.append(data[column])\n",
    "        \n",
    "    dataset = np.transpose(list_of_features)\n",
    "    output_dataset = np.transpose(list_of_outputs)\n",
    "    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = X_scaler.fit_transform(dataset)\n",
    "    Y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    Y_scaler.fit_transform(output_dataset)\n",
    "\n",
    "    return scaled_data, X_scaler, Y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confident-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_and_output(data, number_of_sessions=60):\n",
    "    # number_of_sessions - number of considered previous sessions as an input\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(number_of_sessions, data.shape[0]):\n",
    "        X.append(data[i-number_of_sessions:i, :])\n",
    "        Y.append(data[i, :5])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-bumper",
   "metadata": {},
   "source": [
    "# Testing for the best technical indicators parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amended-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-lancaster",
   "metadata": {},
   "source": [
    "## Checking how algorithm works without techincal indicators (learning based on prices and volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aging-television",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27145, 60, 5)\n",
      "(3017, 60, 5)\n",
      "(27145, 5)\n",
      "(3017, 5)\n",
      "Epoch 1/7\n",
      "849/849 [==============================] - 14s 8ms/step - loss: 0.0135 - val_loss: 5.5805e-04\n",
      "Epoch 2/7\n",
      "849/849 [==============================] - 5s 6ms/step - loss: 5.5525e-04 - val_loss: 4.9114e-04\n",
      "Epoch 3/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 5.0221e-04 - val_loss: 4.6847e-04\n",
      "Epoch 4/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.6461e-04 - val_loss: 4.5416e-04\n",
      "Epoch 5/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.9073e-04 - val_loss: 4.3534e-04\n",
      "Epoch 6/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.5074e-04 - val_loss: 4.6416e-04\n",
      "Epoch 7/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.8541e-04 - val_loss: 4.0307e-04\n",
      "Epoch 1/7\n",
      "849/849 [==============================] - 8s 6ms/step - loss: 0.0055 - val_loss: 4.9376e-04\n",
      "Epoch 2/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 5.1692e-04 - val_loss: 4.5520e-04\n",
      "Epoch 3/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.8782e-04 - val_loss: 4.4255e-04\n",
      "Epoch 4/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.7132e-04 - val_loss: 4.1090e-04\n",
      "Epoch 5/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.5749e-04 - val_loss: 4.1518e-04\n",
      "Epoch 6/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.2905e-04 - val_loss: 4.0322e-04\n",
      "Epoch 7/7\n",
      "849/849 [==============================] - 5s 6ms/step - loss: 4.4159e-04 - val_loss: 4.0325e-04\n",
      "Epoch 1/7\n",
      "849/849 [==============================] - 8s 6ms/step - loss: 0.0303 - val_loss: 9.8590e-04\n",
      "Epoch 2/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 8.8843e-04 - val_loss: 6.8415e-04\n",
      "Epoch 3/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 6.3443e-04 - val_loss: 5.0667e-04\n",
      "Epoch 4/7\n",
      "849/849 [==============================] - 5s 6ms/step - loss: 5.0160e-04 - val_loss: 4.6262e-04\n",
      "Epoch 5/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.9157e-04 - val_loss: 4.3269e-04\n",
      "Epoch 6/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.6283e-04 - val_loss: 4.2595e-04\n",
      "Epoch 7/7\n",
      "849/849 [==============================] - 5s 5ms/step - loss: 4.2562e-04 - val_loss: 4.1512e-04\n"
     ]
    }
   ],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "stocks_base = []\n",
    "results_base = []\n",
    "# prepare data\n",
    "for stock in stock_data_list:\n",
    "    stock = stock.dropna()\n",
    "    scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "    stocks_base.append(scaled_stock)\n",
    "# split for training and validation\n",
    "for stock in stocks_base:\n",
    "    X, Y = prepare_input_and_output(np.array(stock))\n",
    "    X_all = X_all + X\n",
    "    Y_all = Y_all + Y\n",
    "X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_valid.shape)\n",
    "loss_results = []\n",
    "val_loss_results = []\n",
    "for i in range(3):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(Y_train.shape[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "    loss_results.append(history.history['loss'][-1])\n",
    "    val_loss_results.append(history.history['val_loss'][-1])\n",
    "results_base.append({'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_base = pd.DataFrame(results_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "protecting-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.000452  0.000407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-click",
   "metadata": {},
   "source": [
    "### Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "oriental-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27133, 60, 6)\n",
      "(3015, 60, 6)\n",
      "(27133, 5)\n",
      "(3015, 5)\n",
      "Period: 3\n",
      "Epoch 1/7\n",
      "848/848 [==============================] - 8s 6ms/step - loss: 0.0091 - val_loss: 6.1596e-04\n",
      "Epoch 2/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 5.7094e-04 - val_loss: 5.1778e-04\n",
      "Epoch 3/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.2185e-04 - val_loss: 4.6968e-04\n",
      "Epoch 4/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.1063e-04 - val_loss: 5.0525e-04\n",
      "Epoch 5/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 4.7141e-04 - val_loss: 4.3671e-04\n",
      "Epoch 6/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 4.8155e-04 - val_loss: 4.6136e-04\n",
      "Epoch 7/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 4.5671e-04 - val_loss: 4.2620e-04\n",
      "Epoch 1/7\n",
      "848/848 [==============================] - 8s 6ms/step - loss: 0.0127 - val_loss: 5.9502e-04\n",
      "Epoch 2/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 6.1132e-04 - val_loss: 5.0969e-04\n",
      "Epoch 3/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.4138e-04 - val_loss: 4.7495e-04\n",
      "Epoch 4/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.1105e-04 - val_loss: 4.7653e-04\n",
      "Epoch 5/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 5.1564e-04 - val_loss: 4.4990e-04\n",
      "Epoch 6/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 5.1119e-04 - val_loss: 4.4401e-04\n",
      "Epoch 7/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 4.6604e-04 - val_loss: 4.3535e-04\n",
      "Epoch 1/7\n",
      "848/848 [==============================] - 8s 6ms/step - loss: 0.0232 - val_loss: 5.8724e-04\n",
      "Epoch 2/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.5803e-04 - val_loss: 4.9272e-04\n",
      "Epoch 3/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.4382e-04 - val_loss: 4.8005e-04\n",
      "Epoch 4/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.0087e-04 - val_loss: 4.7294e-04\n",
      "Epoch 5/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 4.6749e-04 - val_loss: 4.7064e-04\n",
      "Epoch 6/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 5.2298e-04 - val_loss: 4.2714e-04\n",
      "Epoch 7/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 4.5907e-04 - val_loss: 4.2477e-04\n",
      "(27120, 60, 6)\n",
      "(3014, 60, 6)\n",
      "(27120, 5)\n",
      "(3014, 5)\n",
      "Period: 5\n",
      "Epoch 1/7\n",
      "848/848 [==============================] - 8s 6ms/step - loss: 0.0309 - val_loss: 5.3144e-04\n",
      "Epoch 2/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 6.1813e-04 - val_loss: 4.4883e-04\n",
      "Epoch 3/7\n",
      "848/848 [==============================] - 5s 5ms/step - loss: 5.6366e-04 - val_loss: 4.2106e-04\n",
      "Epoch 4/7\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 5.4413e-04 - val_loss: 4.0764e-04\n",
      "Epoch 5/7\n",
      "848/848 [==============================] - 15s 18ms/step - loss: 4.6593e-04 - val_loss: 3.8653e-04\n",
      "Epoch 6/7\n",
      "848/848 [==============================] - 14s 16ms/step - loss: 5.1760e-04 - val_loss: 4.1085e-04\n",
      "Epoch 7/7\n",
      "848/848 [==============================] - 14s 17ms/step - loss: 4.6647e-04 - val_loss: 3.9609e-04\n",
      "Epoch 1/7\n",
      "848/848 [==============================] - 20s 18ms/step - loss: 0.0135 - val_loss: 5.4865e-04\n",
      "Epoch 2/7\n",
      "318/848 [==========>...................] - ETA: 5s - loss: 6.5997e-04"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-89c9c0cfa3b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mloss_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mval_loss_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    508\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \"\"\"\n\u001b[0;32m   1070\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1037\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "periods = [3, 5, 10, 15, 20, 30, 45, 60, 80, 100, 125, 150, 200]\n",
    "X_all_MA = []\n",
    "Y_all_MA = []\n",
    "results_MA = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all_MA = []\n",
    "    Y_all_MA = []\n",
    "    stocks_MA = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock_MA = get_moving_average(stock, period)\n",
    "        stock_MA = stock_MA.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock_MA.drop(columns='Date'))\n",
    "        stocks_MA.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock_MA in stocks_MA:\n",
    "        X, Y = prepare_input_and_output(np.array(stock_MA))\n",
    "        X_all_MA = X_all_MA + X\n",
    "        Y_all_MA = Y_all_MA + Y\n",
    "    X_all_MA, Y_all_MA = np.array(X_all_MA), np.array(Y_all_MA)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all_MA, Y_all_MA, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('Period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_MA.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_MA = pd.DataFrame(results_MA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Loss function values for diffrent moving average periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_MA['period'], results_MA[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-blind",
   "metadata": {},
   "source": [
    "### RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [3, 5, 7, 10, 14]\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_RSI = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_RSI(stock, period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('Period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_RSI.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_RSI = pd.DataFrame(results_RSI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Loss function values for diffrent RSI periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_RSI['period'], results_RSI[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-consultation",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "results_ROC = []\n",
    "\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "stocks = []\n",
    "# prepare data\n",
    "for stock in stock_data_list:\n",
    "    stock = get_ROC(stock)\n",
    "    stock = stock.dropna()\n",
    "    scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "    stocks.append(scaled_stock)\n",
    "# split for training and validation\n",
    "for stock in stocks:\n",
    "    X, Y = prepare_input_and_output(np.array(stock))\n",
    "    X_all = X_all + X\n",
    "    Y_all = Y_all + Y\n",
    "X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_valid.shape)\n",
    "\n",
    "loss_results = []\n",
    "val_loss_results = []\n",
    "for i in range(3):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(Y_train.shape[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "    loss_results.append(history.history['loss'][-1])\n",
    "    val_loss_results.append(history.history['val_loss'][-1])\n",
    "results_ROC.append({'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_ROC = pd.DataFrame(results_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-needle",
   "metadata": {},
   "source": [
    "## Stochastic oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [3, 5, 7, 10, 14]\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_SO = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_stochastic_oscillator(stock, period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('Period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_SO.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_SO = pd.DataFrame(results_SO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Loss function values for diffrent Stochastic Oscillator periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_SO['period'], results_SO[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-interference",
   "metadata": {},
   "source": [
    "## Moving average of stochastic oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_periods = [3, 5, 7, 10, 14]\n",
    "ma_periods = [2, 3, 4, 5, 6]\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_SO_MA = []\n",
    "\n",
    "\n",
    "for so_period in so_periods:\n",
    "    for ma_period in ma_periods:\n",
    "        X_all = []\n",
    "        Y_all = []\n",
    "        stocks = []\n",
    "        # prepare data\n",
    "        for stock in stock_data_list:\n",
    "            stock = get_SO_moving_average(stock, so_period, ma_period)\n",
    "            stock = stock.dropna()\n",
    "            scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "            stocks.append(scaled_stock)\n",
    "        # split for training and validation\n",
    "        for stock in stocks:\n",
    "            X, Y = prepare_input_and_output(np.array(stock))\n",
    "            X_all = X_all + X\n",
    "            Y_all = Y_all + Y\n",
    "        X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "        print(X_train.shape)\n",
    "        print(X_valid.shape)\n",
    "        print(Y_train.shape)\n",
    "        print(Y_valid.shape)\n",
    "        #\n",
    "        print('Stochastic Oscillator period: ' + str(so_period))\n",
    "        print('Moving average period: ' + str(ma_period))\n",
    "        loss_results = []\n",
    "        val_loss_results = []\n",
    "        for i in range(3):\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "            model.add(Dense(10, activation='relu'))\n",
    "            model.add(Dense(Y_train.shape[1]))\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "            loss_results.append(history.history['loss'][-1])\n",
    "            val_loss_results.append(history.history['val_loss'][-1])\n",
    "        results_SO_MA.append({'so_period': so_period, 'ma_period': ma_period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_SO_MA = pd.DataFrame(results_SO_MA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SO_MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(results_SO_MA['so_period'].unique()))\n",
    "fig.set_size_inches(10,10)\n",
    "fig.suptitle('Loss function values for diffrent perdiods for  Moving average of Stochastic Oscillator', size=24)\n",
    "\n",
    "axes_counter = 0\n",
    "for so_period in results_SO_MA['so_period'].unique():\n",
    "    axes[axes_counter].title.set_text('Stochastic Oscillator period = ' + str(so_period))\n",
    "    axes[axes_counter].plot(results_SO_MA['ma_period'].where(results_SO_MA['so_period']==so_period), results_SO_MA[['loss', 'val_loss']].where(results_SO_MA['so_period']==so_period))\n",
    "    axes[axes_counter].legend(['Training loss',  'Validation loss'])\n",
    "    axes_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-tracy",
   "metadata": {},
   "source": [
    "## MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_periods = [10, 18, 26]\n",
    "short_periods = [6, 9, 12]\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_MACD = []\n",
    "\n",
    "\n",
    "for long_period in long_periods:\n",
    "    for short_period in short_periods:\n",
    "        if long_period <= short_period:\n",
    "            continue\n",
    "        X_all = []\n",
    "        Y_all = []\n",
    "        stocks = []\n",
    "        # prepare data\n",
    "        for stock in stock_data_list:\n",
    "            stock = get_MACD(stock, long_period, short_period)\n",
    "            stock = stock.dropna()\n",
    "            scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "            stocks.append(scaled_stock)\n",
    "        # split for training and validation\n",
    "        for stock in stocks:\n",
    "            X, Y = prepare_input_and_output(np.array(stock))\n",
    "            X_all = X_all + X\n",
    "            Y_all = Y_all + Y\n",
    "        X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "        print(X_train.shape)\n",
    "        print(X_valid.shape)\n",
    "        print(Y_train.shape)\n",
    "        print(Y_valid.shape)\n",
    "        #\n",
    "        print('Long period: ' + str(so_period))\n",
    "        print('Short period: ' + str(ma_period))\n",
    "        loss_results = []\n",
    "        val_loss_results = []\n",
    "        for i in range(3):\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "            model.add(Dense(10, activation='relu'))\n",
    "            model.add(Dense(Y_train.shape[1]))\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "            loss_results.append(history.history['loss'][-1])\n",
    "            val_loss_results.append(history.history['val_loss'][-1])\n",
    "        results_MACD.append({'long_period': long_period, 'short_period': short_period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_MACD = pd.DataFrame(results_MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(results_MACD['long_period'].unique()))\n",
    "fig.set_size_inches(10,10)\n",
    "fig.suptitle('Loss function values for diffrent perdiods of MACD', size=24)\n",
    "\n",
    "axes_counter = 0\n",
    "for long_period in results_MACD['long_period'].unique():\n",
    "    axes[axes_counter].title.set_text('MACD long period = ' + str(long_period))\n",
    "    axes[axes_counter].plot(results_MACD['short_period'].where(results_MACD['long_period']==long_period), results_MACD[['loss', 'val_loss']].where(results_MACD['long_period']==long_period))\n",
    "    axes[axes_counter].legend(['Training loss',  'Validation loss'])\n",
    "    axes_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-judge",
   "metadata": {},
   "source": [
    "## MACD Histogram for diffrent signal lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_period = 26\n",
    "short_period = 12\n",
    "signal_periods = [3, 5, 9, 12]\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_MACD_histogram = []\n",
    "\n",
    "\n",
    "for signal_period in signal_periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_MACD_histogram(stock, long_period, short_period, signal_period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('Signal line period: ' + str(signal_period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_MACD_histogram.append({'long_period': long_period,'short_period': short_period,'signal_period': signal_period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_MACD_histogram = pd.DataFrame(results_MACD_histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MACD_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Loss function values for diffrent MACD signal line periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_MACD_histogram['signal_period'], results_MACD_histogram[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-peace",
   "metadata": {},
   "source": [
    "## Price Oscillator PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_periods = [10, 18, 26]\n",
    "short_periods = [6, 9, 12]\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_PPO = []\n",
    "\n",
    "\n",
    "for long_period in long_periods:\n",
    "    for short_period in short_periods:\n",
    "        if long_period <= short_period:\n",
    "            continue\n",
    "        X_all = []\n",
    "        Y_all = []\n",
    "        stocks = []\n",
    "        # prepare data\n",
    "        for stock in stock_data_list:\n",
    "            stock = get_PPO(stock, long_period, short_period)\n",
    "            stock = stock.dropna()\n",
    "            scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "            stocks.append(scaled_stock)\n",
    "        # split for training and validation\n",
    "        for stock in stocks:\n",
    "            X, Y = prepare_input_and_output(np.array(stock))\n",
    "            X_all = X_all + X\n",
    "            Y_all = Y_all + Y\n",
    "        X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "        print(X_train.shape)\n",
    "        print(X_valid.shape)\n",
    "        print(Y_train.shape)\n",
    "        print(Y_valid.shape)\n",
    "        #\n",
    "        print('Long period: ' + str(so_period))\n",
    "        print('Short period: ' + str(ma_period))\n",
    "        loss_results = []\n",
    "        val_loss_results = []\n",
    "        for i in range(3):\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "            model.add(Dense(10, activation='relu'))\n",
    "            model.add(Dense(Y_train.shape[1]))\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "            loss_results.append(history.history['loss'][-1])\n",
    "            val_loss_results.append(history.history['val_loss'][-1])\n",
    "        results_PPO.append({'long_period': long_period, 'short_period': short_period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_PPO = pd.DataFrame(results_PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(results_PPO['long_period'].unique()))\n",
    "fig.set_size_inches(10,10)\n",
    "fig.suptitle('Loss function values for diffrent perdiods of PPO', size=24)\n",
    "\n",
    "axes_counter = 0\n",
    "for long_period in results_PPO['long_period'].unique():\n",
    "    axes[axes_counter].title.set_text('PPO long period = ' + str(long_period))\n",
    "    axes[axes_counter].plot(results_PPO['short_period'].where(results_PPO['long_period']==long_period), results_PPO[['loss', 'val_loss']].where(results_PPO['long_period']==long_period))\n",
    "    axes[axes_counter].legend(['Training loss',  'Validation loss'])\n",
    "    axes_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-theme",
   "metadata": {},
   "source": [
    "## Triple exponential moving average TEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [3, 5, 9, 12, 15, 21]\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_TEMA = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_TEMA(stock, period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('TEMA period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_TEMA.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_TEMA = pd.DataFrame(results_TEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_TEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Loss function values for diffrent triple exponential moving average periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_TEMA['period'], results_TEMA[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-thailand",
   "metadata": {},
   "source": [
    "## Commodity Channel  Index CGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [5, 10, 15, 20]\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_CGI = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_CGI(stock, period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('CGI period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_CGI.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_CGI = pd.DataFrame(results_CGI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_CGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Loss function values for diffrent CGI periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_CGI['period'], results_CGI[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-brown",
   "metadata": {},
   "source": [
    "## Williams Percent Range WPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [5, 7, 10, 14, 21]\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "results_WPI = []\n",
    "\n",
    "\n",
    "for period in periods:\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    stocks = []\n",
    "    # prepare data\n",
    "    for stock in stock_data_list:\n",
    "        stock = get_Williams_Percent_Range(stock, period)\n",
    "        stock = stock.dropna()\n",
    "        scaled_stock, stock_scaler = scale_data(stock.drop(columns='Date'))\n",
    "        stocks.append(scaled_stock)\n",
    "    # split for training and validation\n",
    "    for stock in stocks:\n",
    "        X, Y = prepare_input_and_output(np.array(stock))\n",
    "        X_all = X_all + X\n",
    "        Y_all = Y_all + Y\n",
    "    X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.1, shuffle=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_valid.shape)\n",
    "    #\n",
    "    print('WPI period: ' + str(period))\n",
    "    loss_results = []\n",
    "    val_loss_results = []\n",
    "    for i in range(3):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(Y_train.shape[1]))\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        history = model.fit(X_train, Y_train, epochs=7, validation_data=(X_valid, Y_valid))\n",
    "        loss_results.append(history.history['loss'][-1])\n",
    "        val_loss_results.append(history.history['val_loss'][-1])\n",
    "    results_WPI.append({'period': period, 'loss': np.mean(loss_results), 'val_loss': np.mean(val_loss_results)})\n",
    "results_WPI = pd.DataFrame(results_WPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_WPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Loss function values for diffrent Williams Percent Range periods', fontsize=24)\n",
    "plt.xlabel('Period', fontsize=25)\n",
    "plt.ylabel('Value', fontsize=18)\n",
    "plt.plot(results_WPI['period'], results_WPI[['loss', 'val_loss']])\n",
    "plt.legend(['Training loss',  'Validation loss'], loc='lower right')\n",
    "plt.axhline(0.000444, color='r', label = 'Loss without indicator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('Time of executing the script: ' + str(round((end - start)/3600, 2)) + 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-system",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
