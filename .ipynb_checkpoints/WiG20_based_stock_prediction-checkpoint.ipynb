{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRPxcxRLeO-3",
    "outputId": "7782bde1-2731-4777-dcd3-8a66ed4f7a86",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3b5e328327d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHrFTNJbAlBk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROqgQ2yGegGJ",
    "outputId": "5aa35344-42bc-4ffc-a7e3-6d89827b2b94"
   },
   "outputs": [],
   "source": [
    "stock_list = ['Alior Bank', 'Allegro', 'Asseco', 'CCC', 'CD Projekt', 'Cyfrowy Polsat', 'Dino Polska', 'JSW', 'KGHM', 'Lotos', 'LPP', 'Orange Polska', 'PEKAO', 'PGE', 'PGNiG', 'PKN Orlen', 'PKO BP', 'PZU', 'Santander', 'Tauron']\n",
    "\n",
    "directory = \"data/\"\n",
    "stock_data_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "    stock_data = stock_data.rename(columns={'Data': 'Date', 'Otwarcie': 'Open', 'Najwyzszy': 'Highest', 'Najnizszy': 'Lowest', 'Zamkniecie': 'Close', 'Wolumen': 'Volume'})\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data.Date)\n",
    "    stock_data_list.append(stock_data)\n",
    "    print('Loaded file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hfNpIWSyMOU"
   },
   "outputs": [],
   "source": [
    "for i in range(len(stock_data_list)):\n",
    "    stock_data_list[i] = stock_data_list[i].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wym9bIyZ3sdA"
   },
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(data, rsi_period=10, so_period=14, so_d_period=4, tema_period=10, cgi_period=20, wpi_period=14):\n",
    "    # rsi_period - number of sessions considered when calculating RSI\n",
    "    # so_period - number of sessions considered when calculating stochastic oscillator K\n",
    "    # so_d_period - numbers of sessions considered when calculating moving average of the stochastic oscillator K\n",
    "    # tema_period - number of sessions considered when calculating TEMA\n",
    "    # cgi_period - number of sessions considered when calculating CGI\n",
    "    # wpi_period - number of sessions considered when calculating Williams' Percent Range\n",
    "\n",
    "    # Moving averages for periods of 10, 30 and 60 days\n",
    "    data['MovingAverage4'] = data['Close'].rolling(4).mean()\n",
    "    data['MovingAverage7'] = data['Close'].rolling(7).mean()\n",
    "    data['MovingAverage20'] = data['Close'].rolling(20).mean()\n",
    "\n",
    "    # Relative Strength Index RSI\n",
    "    increase_difference, decrease_difference = data['Close'].diff(), data['Close'].diff()\n",
    "    increase_difference[increase_difference < 0] = 0\n",
    "    decrease_difference[decrease_difference > 0] = 0\n",
    "    roll_increase = increase_difference.ewm(span = rsi_period).mean()\n",
    "    roll_decrease = decrease_difference.abs().ewm(span = rsi_period).mean()\n",
    "    RS = roll_increase / roll_decrease\n",
    "    data['RSI'] = 100 - (100 / (1 + RS))\n",
    "\n",
    "    # Rate of Change ROC\n",
    "    data['ROC'] = data['Close'].pct_change()\n",
    "\n",
    "    # Stochastic Oscillator K\n",
    "    L14, H14 = data['Close'].rolling(so_period).min(), data['Close'].rolling(so_period).max()\n",
    "    data['K'] = (data['Close'] - L14)/(H14 - L14)\n",
    "\n",
    "    # Moving average of the Stochastic Oscillator D\n",
    "    data['D'] = data['K'].rolling(so_d_period).mean()\n",
    "\n",
    "    # Moving Average Convergence / Divergence MACD\n",
    "    EMA_26 = data['Close'].ewm(26).mean()\n",
    "    EMA_12 = data['Close'].ewm(12).mean()\n",
    "    data['MACD'] = EMA_12 - EMA_26\n",
    "\n",
    "    # MACD Signal Line\n",
    "    data['MACD_Signal'] = data['MACD'].ewm(9).mean()\n",
    "\n",
    "    # MACD histogram\n",
    "    data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "    # Percentage Price Oscillator PPO\n",
    "    data['PPO'] =(EMA_12 - EMA_26)/EMA_26\n",
    "\n",
    "    # Triple Exponential Moving Average TEMA\n",
    "    SEMA = data['Close'].ewm(tema_period).mean()\n",
    "    DEMA = SEMA.ewm(tema_period).mean()\n",
    "    data['TEMA'] = DEMA.ewm(tema_period).mean()\n",
    "\n",
    "    # Commodity Channel Index CGI\n",
    "    typical_price = (data['Highest'] + data['Lowest'] + data['Close']) / 3\n",
    "    MA = typical_price.rolling(cgi_period).mean()\n",
    "    mean_deviation = (MA - typical_price).abs().rolling(cgi_period).mean()\n",
    "    data['CCI'] = (typical_price - MA) / (0.15 * mean_deviation)\n",
    "\n",
    "    # Williams' Percent Range\n",
    "    data['Percent_Range'] = (data['Highest'].rolling(wpi_period).max() - data['Close']) / (data['Highest'].rolling(wpi_period).max() - data['Lowest'].rolling(wpi_period).min())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMbeHn6-hdjU"
   },
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    list_of_features = []\n",
    "    list_of_outputs = []\n",
    "    for column in data.columns:\n",
    "        list_of_features.append(data[column])\n",
    "        if column in ['Open', 'Close', 'Highest', 'Lowest', 'Volume']:\n",
    "            list_of_outputs.append(data[column])\n",
    "        \n",
    "    dataset = np.transpose(list_of_features)\n",
    "    output_dataset = np.transpose(list_of_outputs)\n",
    "    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = X_scaler.fit_transform(dataset)\n",
    "    Y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    Y_scaler.fit_transform(output_dataset)\n",
    "\n",
    "    return scaled_data, X_scaler, Y_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5KwQ3Jqt4qi"
   },
   "outputs": [],
   "source": [
    "def prepare_input_and_output(data, number_of_sessions=60):\n",
    "    # number_of_sessions - number of considered previous sessions as an input\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(number_of_sessions, data.shape[0]):\n",
    "        X.append(data[i-number_of_sessions:i, :])\n",
    "        Y.append(data[i, :5])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTwWMPg217MJ"
   },
   "source": [
    "**Lerning based on prices and volume only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdozVb0r166b"
   },
   "outputs": [],
   "source": [
    "scaled_stocks_basic = []\n",
    "X_scalers_basic = []\n",
    "Y_scalers_basic = []\n",
    "\n",
    "for i in range(len(stock_data_list)):\n",
    "    # Date is dropped as it isn't considered for learning\n",
    "    scaled_stock, X_scaler, Y_scaler = scale_data(stock_data_list[i].drop(columns='Date'))\n",
    "  \n",
    "    scaled_stocks_basic.append(scaled_stock)\n",
    "    X_scalers_basic.append(X_scaler)\n",
    "    Y_scalers_basic.append(Y_scaler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cR5gBYy22ZUM",
    "outputId": "21a9fea4-878f-49bd-8821-e0e9fbce5aaa"
   },
   "outputs": [],
   "source": [
    "X_all_basic = []\n",
    "Y_all_basic = []\n",
    "scaled_X_list_basic = []\n",
    "scaled_Y_list_basic = []\n",
    "\n",
    "for stock in scaled_stocks_basic:\n",
    "    X, Y = prepare_input_and_output(stock, 60)\n",
    "    scaled_X_list_basic.append(np.array(X))\n",
    "    scaled_Y_list_basic.append(np.array(Y))\n",
    "    X_all_basic = X_all_basic + X\n",
    "    Y_all_basic = Y_all_basic + Y\n",
    "\n",
    "X_all_basic, Y_all_basic = np.array(X_all_basic), np.array(Y_all_basic)\n",
    "print('Shape of input matrix: ' + str(X_all_basic.shape))\n",
    "print('Shape of output matrix: ' + str(Y_all_basic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpQC3Vld3Cky",
    "outputId": "21c85455-4fd7-404d-f133-f58f7933dc5b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_basic, X_valid_basic, Y_train_basic, Y_valid_basic = train_test_split(X_all_basic, Y_all_basic, test_size=0.05, shuffle=False)\n",
    "\n",
    "print('Shape of training input matrix: ' + str(X_train_basic.shape))\n",
    "print('Shape of training output matrix: ' + str(Y_train_basic.shape))\n",
    "print('Shape of validation input matrix: ' + str(X_valid_basic.shape))\n",
    "print('Shape of validation output matrix: ' + str(Y_valid_basic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "id": "GKgpalBO3MOB",
    "outputId": "19421dbe-d999-4565-ea03-4a4e318890f2"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=[X_train_basic.shape[1], X_train_basic.shape[2]]))#, return_sequences=True))\n",
    "#model.add(LSTM(64))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(Y_train_basic.shape[1]))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train_basic, Y_train_basic, epochs=10, validation_data=(X_valid_basic, Y_valid_basic))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNI08uGj3SD6",
    "outputId": "22549d09-a11f-4a5d-e52d-1bec890ffae7"
   },
   "outputs": [],
   "source": [
    "# Evaluation of the results is made on closing prices - it gives the most information about the usefullness of the model\n",
    "\n",
    "predictions_basic = model.predict(X_valid_basic)\n",
    "predictions_basic = Y_scalers_basic[0].inverse_transform(predictions_basic)\n",
    "real_output_basic = Y_scalers_basic[0].inverse_transform(Y_valid_basic)\n",
    "\n",
    "predicted_data_basic = pd.DataFrame(predictions_basic, columns=['Open_predicted', 'Close_predicted', 'Highest_predicted', 'Lowest_predicted', 'Volume_predicted'])\n",
    "real_data_basic = pd.DataFrame(real_output_basic, columns=['Open_real', 'Close_real', 'Highest_real', 'Lowest_real', 'Volume_real'])\n",
    "predictions_basic = pd.concat([real_data_basic, predicted_data_basic], axis=1)\n",
    "print(predictions_basic[['Close_real', 'Close_predicted']].tail(20))\n",
    "\n",
    "predictions_basic['Close_difference'] = abs(predictions_basic['Close_real'] - predictions_basic['Close_predicted'])\n",
    "predictions_basic['Close_difference_percent'] = abs(predictions_basic['Close_real'] - predictions_basic['Close_predicted'])/predictions_basic['Close_real'] * 100\n",
    "\n",
    "previous_Close_basic = predictions_basic['Close_real'].shift(-1)\n",
    "Naive_forcast_MAPE = pd.df(abs(predictions_basic['Close_real'] - previous_Close_basic)/predictions_basic['Close_real'] * 100).mean()\n",
    "\n",
    "MAPE_basic = predictions_basic['Close_difference_percent'].mean()\n",
    "\n",
    "print('Naive forcast MAPE: ' + str(round(Naive_forcast_MAPE,2)) + '%'')\n",
    "print('Predictions MAPE: ' + str(round(MAPE_basic,2)) + '%'')\n",
    "\n",
    "\n",
    "# mae = np.mean(np.abs((predictions[:,1] - scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[:,1])))\n",
    "# naive_forcast_difference = [abs(scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[i + 1,1] - scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[i,1]) for i in range(scaled_Y_list_basic[0].shape[0] - 1)]\n",
    "# naive_forcast_mae = sum(naive_forcast_difference) / len(naive_forcast_difference)\n",
    "# mape = np.mean((np.abs((predictions[:,1] - scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[:,1]))/scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[:,1])*100)\n",
    "# naive_forcast_mape = (naive_forcast_mae/scalers_basic[0].inverse_transform(scaled_Y_list_basic[0])[:,1].mean()) * 100\n",
    "\n",
    "# print('Prediction mean absolute error: ' + str(mae))\n",
    "# print('Naive forcast mean absolute error: ' + str(naive_forcast_mae))\n",
    "\n",
    "# print('Prediction mean absolute percentage error: ' + str(round(mape, 2)) + '%')\n",
    "# print('Naive forcast mean absolute percentage error: ' + str(round(naive_forcast_mape, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa15Drwo2GiE"
   },
   "source": [
    "**Lerning based on prices, volume and technical indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "GPA-Ze1S5lH-",
    "outputId": "481befc5-9cb4-43a2-e5ef-ece0ea2cb53a"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "# calculate_technical_indicators parameters -> data, rsi_period=10, so_period=14, so_d_period=4, tema_period=10, cgi_period=20, wpi_period=14\n",
    "for i in range(len(stock_data_list)):\n",
    "    stock_data_list[i] = calculate_technical_indicators(stock_data_list[i], rsi_period=60, so_period=5, so_d_period=3, tema_period=4, cgi_period=4, wpi_period=4)\n",
    "    # moving averages return NaN when the considered period is greater than available data, these rows need to be dropped\n",
    "    stock_data_list[i] = stock_data_list[i].dropna()\n",
    "\n",
    "display.display(stock_data_list[0].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6SLv9Gn9J33"
   },
   "outputs": [],
   "source": [
    "scaled_stocks = []\n",
    "X_scalers = []\n",
    "Y_scalers = []\n",
    "\n",
    "for i in range(len(stock_data_list)):\n",
    "    # Date is dropped as it isn't considered for learning\n",
    "    scaled_stock, X_scaler, Y_scaler = scale_data(stock_data_list[i].drop(columns='Date'))\n",
    "    scaled_stocks.append(scaled_stock)\n",
    "    X_scalers.append(X_scaler)\n",
    "    Y_scalers.append(Y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcEx0942taj9",
    "outputId": "d75340a2-9821-470b-8374-d1795f00a034"
   },
   "outputs": [],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "scaled_X_list = []\n",
    "scaled_Y_list = []\n",
    "\n",
    "for stock in scaled_stocks:\n",
    "  X, Y = prepare_input_and_output(stock)\n",
    "  scaled_X_list.append(np.array(X))\n",
    "  scaled_Y_list.append(np.array(Y))\n",
    "  X_all = X_all + X\n",
    "  Y_all = Y_all + Y\n",
    "\n",
    "X_all, Y_all = np.array(X_all), np.array(Y_all)\n",
    "print('Shape of input matrix: ' + str(X_all.shape))\n",
    "print('Shape of output matrix: ' + str(Y_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3-GXVfNuo35",
    "outputId": "fb8890c3-9cc0-4ec0-f49b-ca83806c3be7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_all, Y_all, test_size=0.05, shuffle=False)\n",
    "\n",
    "print('Shape of training input matrix: ' + str(X_train.shape))\n",
    "print('Shape of training output matrix: ' + str(Y_train.shape))\n",
    "print('Shape of validation input matrix: ' + str(X_valid.shape))\n",
    "print('Shape of validation output matrix: ' + str(Y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kB4Cppg_Hep"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToVHaihd57qh",
    "outputId": "b02dcf45-25b3-47b6-e01f-a689ab8d3ee5"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32,input_shape=[X_train.shape[1], X_train.shape[2]]))#, return_sequences=True))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(Y_train.shape[1]))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=10, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG9qaeTUCkkZ",
    "outputId": "e86c230c-ca93-4422-96b7-29b44bfcf1cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of the results is made on closing prices - it gives the most information about the usefullness of the model\n",
    "\n",
    "predictions = model.predict(X_valid)\n",
    "predictions = Y_scalers[0].inverse_transform(predictions)\n",
    "real_output = Y_scalers[0].inverse_transform(Y_valid)\n",
    "\n",
    "predicted_data = pd.DataFrame(predictions, columns=['Open_predicted', 'Close_predicted', 'Highest_predicted', 'Lowest_predicted', 'Volume_predicted'])\n",
    "real_data = pd.DataFrame(real_output, columns=['Open_real', 'Close_real', 'Highest_real', 'Lowest_real', 'Volume_real'])\n",
    "predictions = pd.concat([real_data, predicted_data], axis=1)\n",
    "print(predictions[['Close_real', 'Close_predicted']].tail(20))\n",
    "\n",
    "predictions['Close_difference'] = abs(predictions['Close_real'] - predictions['Close_predicted'])\n",
    "predictions['Close_difference_percent'] = abs(predictions['Close_real'] - predictions['Close_predicted'])/predictions['Close_real'] * 100\n",
    "\n",
    "MAPE = predictions['Close_difference_percent'].mean()\n",
    "\n",
    "print('Predictions MAPE: ' + str(MAPE))\n",
    "\n",
    "# mae = np.mean(np.abs((predictions[:,1] - scalers[0].inverse_transform(scaled_Y_list[0])[:,1])))\n",
    "# naive_forcast_difference = [abs(scalers[0].inverse_transform(scaled_Y_list[0])[i + 1,1] - scalers[0].inverse_transform(scaled_Y_list[0])[i,1]) for i in range(scaled_Y_list[0].shape[0] - 1)]\n",
    "# naive_forcast_mae = sum(naive_forcast_difference) / len(naive_forcast_difference)\n",
    "# mape = np.mean((np.abs((predictions[:,1] - scalers[0].inverse_transform(scaled_Y_list[0])[:,1]))/scalers[0].inverse_transform(scaled_Y_list[0])[:,1])*100)\n",
    "# naive_forcast_mape = (naive_forcast_mae/scalers[0].inverse_transform(scaled_Y_list[0])[:,1].mean()) * 100\n",
    "\n",
    "# # print('Prediction mean absolute error: ' + str(mae))\n",
    "# # print('Naive forcast mean absolute error: ' + str(naive_forcast_mae))\n",
    "\n",
    "# print('Prediction mean absolute percentage error: ' + str(round(mape, 2)) + '%')\n",
    "# print('Naive forcast mean absolute percentage error: ' + str(round(naive_forcast_mape, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WiG20 based stock prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
